loki-distributed:
  fullnameOverride: loki

  loki:
    # -- Common command override for all pods (except gateway)
    command: null
    # -- The number of old ReplicaSets to retain to allow rollback
    revisionHistoryLimit: 3
    # -- The SecurityContext for Loki pods
    podSecurityContext:
      fsGroup: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      runAsUser: 1001
    # -- Adds the appProtocol field to the memberlist service. This allows memberlist to work with istio protocol selection. Ex: "http" or "tcp"
    appProtocol: ""
    # -- Common annotations for all loki services
    serviceAnnotations: {}
    server:
      http_listen_port: 3100
    # -- Config file contents for Loki
    config: |
      auth_enabled: false

      server:
        {{- toYaml .Values.loki.server | nindent 6 }}

      common:
        compactor_address: http://{{ include "loki.compactorFullname" . }}:3100

      distributor:
        ring:
          kvstore:
            store: memberlist

      memberlist:
        join_members:
          - {{ include "loki.fullname" . }}-memberlist

      ingester:
        lifecycler:
          ring:
            kvstore:
              store: memberlist
            replication_factor: 1
        chunk_idle_period: 30m
        chunk_block_size: 262144
        chunk_encoding: snappy
        chunk_retain_period: 1m
        max_transfer_retries: 0
        wal:
          dir: /var/loki/wal

      limits_config:
        enforce_metric_name: false
        reject_old_samples: true
        reject_old_samples_max_age: 168h
        max_cache_freshness_per_query: 10m
        split_queries_by_interval: 15m

      {{- if .Values.loki.schemaConfig}}
      schema_config:
      {{- toYaml .Values.loki.schemaConfig | nindent 2}}
      {{- end}}
      {{- if .Values.loki.storageConfig}}
      storage_config:
      {{- if .Values.indexGateway.enabled}}
      {{- $indexGatewayClient := dict "server_address" (printf "dns:///%s:9095" (include "loki.indexGatewayFullname" .)) }}
      {{- $_ := set .Values.loki.storageConfig.boltdb_shipper "index_gateway_client" $indexGatewayClient }}
      {{- end}}
      {{- toYaml .Values.loki.storageConfig | nindent 2}}
      {{- if .Values.memcachedIndexQueries.enabled }}
        index_queries_cache_config:
          memcached_client:
            addresses: dnssrv+_memcached-client._tcp.{{ include "loki.memcachedIndexQueriesFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}
            consistent_hash: true
      {{- end}}
      {{- end}}

      runtime_config:
        file: /var/{{ include "loki.name" . }}-runtime/runtime.yaml

      chunk_store_config:
        max_look_back_period: 0s
        {{- if .Values.memcachedChunks.enabled }}
        chunk_cache_config:
          embedded_cache:
            enabled: false
          memcached_client:
            consistent_hash: true
            addresses: dnssrv+_memcached-client._tcp.{{ include "loki.memcachedChunksFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}
        {{- end }}
        {{- if .Values.memcachedIndexWrites.enabled }}
        write_dedupe_cache_config:
          memcached_client:
            consistent_hash: true
            addresses: dnssrv+_memcached-client._tcp.{{ include "loki.memcachedIndexWritesFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}
        {{- end }}

      table_manager:
        retention_deletes_enabled: false
        retention_period: 0s

      query_range:
        align_queries_with_step: true
        max_retries: 5
        cache_results: true
        results_cache:
          cache:
            {{- if .Values.memcachedFrontend.enabled }}
            memcached_client:
              addresses: dnssrv+_memcached-client._tcp.{{ include "loki.memcachedFrontendFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}
              consistent_hash: true
            {{- else }}
            embedded_cache:
              enabled: true
              ttl: 24h
            {{- end }}

      frontend_worker:
        {{- if .Values.queryScheduler.enabled }}
        scheduler_address: {{ include "loki.querySchedulerFullname" . }}:9095
        {{- else }}
        frontend_address: {{ include "loki.queryFrontendFullname" . }}-headless:9095
        {{- end }}

      frontend:
        log_queries_longer_than: 5s
        compress_responses: true
        {{- if .Values.queryScheduler.enabled }}
        scheduler_address: {{ include "loki.querySchedulerFullname" . }}:9095
        {{- end }}
        tail_proxy_url: http://{{ include "loki.querierFullname" . }}:3100

      compactor:
        shared_store: filesystem

      # ruler:
      #   storage:
      #     type: local
      #     local:
      #       directory: /etc/loki/rules
      #   ring:
      #     kvstore:
      #       store: memberlist
      #   rule_path: /tmp/loki/scratch
      #   alertmanager_url: https://alertmanager.xx
      #   external_url: https://alertmanager.xx

    # -- Check https://grafana.com/docs/loki/latest/configuration/#schema_config for more info on how to configure schemas
    schemaConfig:
      configs:
      - from: 2020-09-07
        store: boltdb-shipper
        object_store: filesystem
        schema: v11
        index:
          prefix: loki_index_
          period: 24h

    # -- Check https://grafana.com/docs/loki/latest/configuration/#storage_config for more info on how to configure storages
    storageConfig:
      boltdb_shipper:
        shared_store: filesystem
        active_index_directory: /var/loki/index
        cache_location: /var/loki/cache
        cache_ttl: 168h
      filesystem:
        directory: /var/loki/chunks

    # -- Structured loki configuration, takes precedence over `loki.config`, `loki.schemaConfig`, `loki.storageConfig`
    structuredConfig: {}

  # -- Provides a reloadable runtime configuration file for some specific configuration
  runtimeConfig: {}

  # ServiceMonitor configuration
  serviceMonitor:
    # -- If enabled, ServiceMonitor resources for Prometheus Operator are created
    enabled: true
    # -- Alternative namespace for ServiceMonitor resources
    namespace: null
    # -- Namespace selector for ServiceMonitor resources
    namespaceSelector: {}
    # -- ServiceMonitor annotations
    annotations: {}
    # -- Additional ServiceMonitor labels
    labels: {}

  # Rules for the Prometheus Operator
  prometheusRule:
    # -- If enabled, a PrometheusRule resource for Prometheus Operator is created
    enabled: true
    # -- Alternative namespace for the PrometheusRule resource
    namespace: null
    # -- PrometheusRule annotations
    annotations: {}
    # -- Additional PrometheusRule labels
    labels: {}
    # -- Contents of Prometheus rules file
    groups:
     - name: loki_rules
       rules:
         - expr: histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket[1m]))
             by (le, cluster, job))
           record: cluster_job:loki_request_duration_seconds:99quantile
         - expr: histogram_quantile(0.50, sum(rate(loki_request_duration_seconds_bucket[1m]))
             by (le, cluster, job))
           record: cluster_job:loki_request_duration_seconds:50quantile
         - expr: sum(rate(loki_request_duration_seconds_sum[1m])) by (cluster, job) / sum(rate(loki_request_duration_seconds_count[1m]))
             by (cluster, job)
           record: cluster_job:loki_request_duration_seconds:avg
         - expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, cluster, job)
           record: cluster_job:loki_request_duration_seconds_bucket:sum_rate
         - expr: sum(rate(loki_request_duration_seconds_sum[1m])) by (cluster, job)
           record: cluster_job:loki_request_duration_seconds_sum:sum_rate
         - expr: sum(rate(loki_request_duration_seconds_count[1m])) by (cluster, job)
           record: cluster_job:loki_request_duration_seconds_count:sum_rate
         - expr: histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket[1m]))
             by (le, cluster, job, route))
           record: cluster_job_route:loki_request_duration_seconds:99quantile
         - expr: histogram_quantile(0.50, sum(rate(loki_request_duration_seconds_bucket[1m]))
             by (le, cluster, job, route))
           record: cluster_job_route:loki_request_duration_seconds:50quantile
         - expr: sum(rate(loki_request_duration_seconds_sum[1m])) by (cluster, job, route)
             / sum(rate(loki_request_duration_seconds_count[1m])) by (cluster, job, route)
           record: cluster_job_route:loki_request_duration_seconds:avg
         - expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, cluster, job,
             route)
           record: cluster_job_route:loki_request_duration_seconds_bucket:sum_rate
         - expr: sum(rate(loki_request_duration_seconds_sum[1m])) by (cluster, job, route)
           record: cluster_job_route:loki_request_duration_seconds_sum:sum_rate
         - expr: sum(rate(loki_request_duration_seconds_count[1m])) by (cluster, job, route)
           record: cluster_job_route:loki_request_duration_seconds_count:sum_rate
         - expr: histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket[1m]))
             by (le, cluster, namespace, job, route))
           record: cluster_namespace_job_route:loki_request_duration_seconds:99quantile
         - expr: histogram_quantile(0.50, sum(rate(loki_request_duration_seconds_bucket[1m]))
             by (le, cluster, namespace, job, route))
           record: cluster_namespace_job_route:loki_request_duration_seconds:50quantile
         - expr: sum(rate(loki_request_duration_seconds_sum[1m])) by (cluster, namespace,
             job, route) / sum(rate(loki_request_duration_seconds_count[1m])) by (cluster,
             namespace, job, route)
           record: cluster_namespace_job_route:loki_request_duration_seconds:avg
         - expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, cluster, namespace,
             job, route)
           record: cluster_namespace_job_route:loki_request_duration_seconds_bucket:sum_rate
         - expr: sum(rate(loki_request_duration_seconds_sum[1m])) by (cluster, namespace,
             job, route)
           record: cluster_namespace_job_route:loki_request_duration_seconds_sum:sum_rate
         - expr: sum(rate(loki_request_duration_seconds_count[1m])) by (cluster, namespace,
             job, route)
           record: cluster_namespace_job_route:loki_request_duration_seconds_count:sum_rate

  # Configuration for the ingester
  ingester:
    # -- Kind of deployment [StatefulSet/Deployment]
    kind: StatefulSet
    # -- Number of replicas for the ingester
    replicas: 1
    # -- Command to execute instead of defined in Docker image
    command: null
    # -- The name of the PriorityClass for ingester pods
    extraArgs: []
    # -- Environment variables to add to the ingester pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the ingester pods
    # -- Init containers to add to the ingester pods
    initContainers: []
    # -- Pod Disruption Budget maxUnavailable
    maxUnavailable: null
    # -- Node selector for ingester pods
    nodeSelector: {}
    # -- Tolerations for ingester pods
    livenessProbe: {}
    persistence:
      # -- Enable creating PVCs which is required when using boltdb-shipper
      enabled: false
      # -- Use emptyDir with ramdisk for storage. **Please note that all data in ingester will be lost on pod restart**
      inMemory: false
        # -- List of the ingester PVCs
        # @notationType -- list
      claims:
        - name: data
          size: 10Gi
          #   -- Storage class to be used.
          #   If defined, storageClassName: <storageClass>.
          #   If set to "-", storageClassName: "", which disables dynamic provisioning.
          #   If empty or set to null, no storageClassName spec is
          #   set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
          storageClass: null
        # - name: wal
        #   size: 150Gi

    # -- Adds the appProtocol field to the ingester service. This allows ingester to work with istio protocol selection.
    appProtocol:
      # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
      grpc: ""

  # Configuration for the distributor
  distributor:
    # -- Number of replicas for the distributor
    replicas: 1
    # -- Command to execute instead of defined in Docker image
    command: null
    # -- The name of the PriorityClass for distributor pods
    nodeSelector: {}
    # -- Tolerations for distributor pods
    tolerations: []
      # -- Adds the appProtocol field to the distributor service. This allows distributor to work with istio protocol selection.
    appProtocol:
      # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
      grpc: ""

  # Configuration for the querier
  querier:
    # -- Number of replicas for the querier
    replicas: 1
    nodeSelector: {}
    # -- Tolerations for querier pods
    tolerations: []
    # -- DNSConfig for querier pods
    dnsConfig: {}
    persistence:
      # -- Enable creating PVCs for the querier cache
      enabled: false
      # -- Size of persistent disk
      size: 10Gi
      # -- Storage class to be used.
      # If defined, storageClassName: <storageClass>.
      # If set to "-", storageClassName: "", which disables dynamic provisioning.
      # If empty or set to null, no storageClassName spec is
      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
      storageClass: null
      # -- Annotations for querier PVCs
      annotations: {}
    # -- Adds the appProtocol field to the querier service. This allows querier to work with istio protocol selection.
    appProtocol:
      # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
      grpc: ""

  # Configuration for the query-frontend
  queryFrontend:
    # -- Number of replicas for the query-frontend
    replicas: 1
    nodeSelector: {}
    # -- Tolerations for query-frontend pods
    tolerations: []
    # -- Adds the appProtocol field to the queryFrontend service. This allows queryFrontend to work with istio protocol selection.
    appProtocol:
      # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
      grpc: ""

  # Configuration for the table-manager
  tableManager:
    # -- Specifies whether the table-manager should be enabled
    enabled: false
    image:
      # -- The Docker registry for the table-manager image. Overrides `loki.image.registry`
      registry: null
      # -- Docker image repository for the table-manager image. Overrides `loki.image.repository`
      repository: null
      # -- Docker image tag for the table-manager image. Overrides `loki.image.tag`
      tag: null
    # -- Command to execute instead of defined in Docker image
    command: null
    # -- The name of the PriorityClass for table-manager pods
    priorityClassName: null
    # -- Labels for table-manager pods
    podLabels: {}
    # -- Annotations for table-manager pods
    podAnnotations: {}
    # -- Labels for table-manager service
    serviceLabels: {}
    # -- Additional CLI args for the table-manager
    extraArgs: []
    # -- Environment variables to add to the table-manager pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the table-manager pods
    extraEnvFrom: []
    # -- Volume mounts to add to the table-manager pods
    extraVolumeMounts: []
    # -- Volumes to add to the table-manager pods
    extraVolumes: []
    # -- Resource requests and limits for the table-manager
    resources: {}
    # -- Containers to add to the table-manager pods
    extraContainers: []
    # -- Grace period to allow the table-manager to shutdown before it is killed
    terminationGracePeriodSeconds: 30
    # -- Affinity for table-manager pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Hard node and soft zone anti-affinity
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "loki.tableManagerSelectorLabels" . | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "loki.tableManagerSelectorLabels" . | nindent 12 }}
              topologyKey: failure-domain.beta.kubernetes.io/zone
    # -- Node selector for table-manager pods
    nodeSelector: {}
    # -- Tolerations for table-manager pods
    tolerations: []

  # Configuration for the gateway
  gateway:
    # -- Specifies whether the gateway should be enabled
    enabled: true
    # -- Number of replicas for the gateway
    replicas: 1
    # -- Enable logging of 2xx and 3xx HTTP requests
    verboseLogging: true
    # -- See `kubectl explain deployment.spec.strategy` for more,
    # ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
    extraArgs: []
    # -- Environment variables to add to the gateway pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the gateway pods
    extraEnvFrom: []
    # -- Volumes to add to the gateway pods
    extraVolumes: []
    # -- Volume mounts to add to the gateway pods
    extraVolumeMounts: []
    # -- The SecurityContext for gateway containers
    podSecurityContext:
      fsGroup: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      runAsUser: 1001
    # -- The SecurityContext for gateway containers
    # -- Resource requests and limits for the gateway
    resources: {}
    # -- Containers to add to the gateway pods
    extraContainers: []
    # -- DNSConfig for gateway pods
    dnsConfig: {}
    # Gateway service configuration
    service:
      # -- Port of the gateway service
      port: 80
      # -- Type of the gateway service
      type: ClusterIP
      # -- ClusterIP of the gateway service
      clusterIP: null
      # -- Node port if service type is NodePort
      nodePort: null
      # -- Load balancer IPO address if service type is LoadBalancer
      loadBalancerIP: null
      # -- Load balancer allow traffic from CIDR list if service type is LoadBalancer
      loadBalancerSourceRanges: []
      # -- Set appProtocol for the service
      appProtocol: null
      # -- Annotations for the gateway service
      annotations: {}
      # -- Labels for gateway service
      labels: {}
    # Gateway ingress configuration
    ingress:
      # -- Specifies whether an ingress for the gateway should be created
      enabled: false
      # -- Ingress Class Name. MAY be required for Kubernetes versions >= 1.18
      # For example: `ingressClassName: nginx`
      ingressClassName: ''

      # -- Annotations for the gateway ingress
      annotations: {}
      # -- Hosts configuration for the gateway ingress
      hosts:
        - host: gateway.loki.example.com
          paths:
            - path: /
              # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers
              # pathType: Prefix
      # -- TLS configuration for the gateway ingress
      tls: []
      # tls:
      #   - secretName: loki-gateway-tls
      #     hosts:
      #       - gateway.loki.example.com


  # Configuration for the compactor
  compactor:
    # -- Specifies whether compactor should be enabled
    enabled: true
    persistence:
      # -- Enable creating PVCs for the compactor
      enabled: false
      # -- Size of persistent disk
      size: 10Gi
      # -- Storage class to be used.
      # If defined, storageClassName: <storageClass>.
      # If set to "-", storageClassName: "", which disables dynamic provisioning.
      # If empty or set to null, no storageClassName spec is
      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
      storageClass: null
      # -- Annotations for compactor PVCs
      annotations: {}
    serviceAccount:
      create: true
      # -- The name of the ServiceAccount to use for the compactor.
      # If not set and create is true, a name is generated by appending
      # "-compactor" to the common ServiceAccount.
      name: null
      # -- Image pull secrets for the compactor service account
      imagePullSecrets: []
      # -- Annotations for the compactor service account
      annotations: {}
      # -- Set this toggle to false to opt out of automounting API credentials for the service account
      automountServiceAccountToken: true

  # Configuration for the ruler
  ruler:
    # -- Specifies whether the ruler should be enabled
    enabled: true
    # -- Kind of deployment [StatefulSet/Deployment]
    kind: Deployment
    # -- Number of replicas for the ruler
    replicas: 1
    # -- DNSConfig for ruler pods
    dnsConfig: {}
    persistence:
      # -- Enable creating PVCs which is required when using recording rules
      enabled: false
      # -- Size of persistent disk
      size: 10Gi
      # -- Storage class to be used.
      # If defined, storageClassName: <storageClass>.
      # If set to "-", storageClassName: "", which disables dynamic provisioning.
      # If empty or set to null, no storageClassName spec is
      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
      storageClass: null
      # -- Annotations for ruler PVCs
      annotations: {}
    # -- Directories containing rules files
    directories: {}
      # tenant_foo:
      #   rules1.txt: |
      #     groups:
      #       - name: should_fire
      #         rules:
      #           - alert: HighPercentageError
      #             expr: |
      #               sum(rate({app="foo", env="production"} |= "error" [5m])) by (job)
      #                 /
      #               sum(rate({app="foo", env="production"}[5m])) by (job)
      #                 > 0.05
      #             for: 10m
      #             labels:
      #               severity: warning
      #             annotations:
      #               summary: High error rate
      #       - name: credentials_leak
      #         rules:
      #           - alert: http-credentials-leaked
      #             annotations:
      #               message: "{{ $labels.job }} is leaking http basic auth credentials."
      #             expr: 'sum by (cluster, job, pod) (count_over_time({namespace="prod"} |~ "http(s?)://(\\w+):(\\w+)@" [5m]) > 0)'
      #             for: 10m
      #             labels:
      #               severity: critical
      #   rules2.txt: |
      #     groups:
      #       - name: example
      #         rules:
      #         - alert: HighThroughputLogStreams
      #           expr: sum by(container) (rate({job=~"loki-dev/.*"}[1m])) > 1000
      #           for: 2m
      # tenant_bar:
      #   rules1.txt: |
      #     groups:
      #       - name: should_fire
      #         rules:
      #           - alert: HighPercentageError
      #             expr: |
      #               sum(rate({app="foo", env="production"} |= "error" [5m])) by (job)
      #                 /
      #               sum(rate({app="foo", env="production"}[5m])) by (job)
      #                 > 0.05
      #             for: 10m
      #             labels:
      #               severity: warning
      #             annotations:
      #               summary: High error rate
      #       - name: credentials_leak
      #         rules:
      #           - alert: http-credentials-leaked
      #             annotations:
      #               message: "{{ $labels.job }} is leaking http basic auth credentials."
      #             expr: 'sum by (cluster, job, pod) (count_over_time({namespace="prod"} |~ "http(s?)://(\\w+):(\\w+)@" [5m]) > 0)'
      #             for: 10m
      #             labels:
      #               severity: critical
      #   rules2.txt: |
      #     groups:
      #       - name: example
      #         rules:
      #         - alert: HighThroughputLogStreams
      #           expr: sum by(container) (rate({job=~"loki-dev/.*"}[1m])) > 1000
      #           for: 2m



  memcachedExporter:
    enabled: true

  memcachedChunks:
    # -- Specifies whether the Memcached chunks cache should be enabled
    enabled: true
    # -- Number of replicas for memcached-chunks
    replicas: 1
    extraArgs:
      - -I 32m
    tolerations: []
    persistence:
      # -- Enable creating PVCs which will persist cached data through restarts
      enabled: false
      # -- Size of persistent or memory disk
      size: 10Gi
      # -- Storage class to be used.
      # If defined, storageClassName: <storageClass>.
      # If set to "-", storageClassName: "", which disables dynamic provisioning.
      # If empty or set to null, no storageClassName spec is
      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
      storageClass: null

  memcachedFrontend:
    # -- Specifies whether the Memcached frontend cache should be enabled
    enabled: false
    # -- Number of replicas for memcached-frontend
    replicas: 1
    # -- The name of the PriorityClass for memcached-frontend pods
    priorityClassName: null
    # -- Labels for memcached-frontend pods
    podLabels: {}
    # -- Annotations for memcached-frontend pods
    podAnnotations: {}
    # -- Labels for memcached-frontend service
    serviceLabels: {}
    # -- Additional CLI args for memcached-frontend
    extraArgs:
      - -I 32m
    # -- Environment variables to add to memcached-frontend pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to memcached-frontend pods
    extraEnvFrom: []
    # -- Resource requests and limits for memcached-frontend
    resources: {}
    # -- Containers to add to the memcached-frontend pods
    extraContainers: []
    # -- Grace period to allow memcached-frontend to shutdown before it is killed
    terminationGracePeriodSeconds: 30
    # -- Affinity for memcached-frontend pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Hard node and soft zone anti-affinity
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "loki.memcachedFrontendSelectorLabels" . | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "loki.memcachedFrontendSelectorLabels" . | nindent 12 }}
              topologyKey: failure-domain.beta.kubernetes.io/zone
    # -- Pod Disruption Budget maxUnavailable
    maxUnavailable: 1
    # -- Node selector for memcached-frontend pods
    nodeSelector: {}
    # -- Tolerations for memcached-frontend pods
    tolerations: []
    persistence:
      # -- Enable creating PVCs which will persist cached data through restarts
      enabled: false
      # -- Size of persistent or memory disk
      size: 10Gi
      # -- Storage class to be used.
      # If defined, storageClassName: <storageClass>.
      # If set to "-", storageClassName: "", which disables dynamic provisioning.
      # If empty or set to null, no storageClassName spec is
      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
      storageClass: null

  memcachedIndexQueries:
    # -- Specifies whether the Memcached index queries cache should be enabled
    enabled: true

  memcachedIndexWrites:
    # -- Specifies whether the Memcached index writes cache should be enabled
    enabled: true

  networkPolicy:
    # -- Specifies whether Network Policies should be created
    enabled: false
    metrics:
      # -- Specifies the Pods which are allowed to access the metrics port.
      # As this is cross-namespace communication, you also need the namespaceSelector.
      podSelector: {}
      # -- Specifies the namespaces which are allowed to access the metrics port
      namespaceSelector: {}
      # -- Specifies specific network CIDRs which are allowed to access the metrics port.
      # In case you use namespaceSelector, you also have to specify your kubelet networks here.
      # The metrics ports are also used for probes.
      cidrs: []
    ingress:
      # -- Specifies the Pods which are allowed to access the http port.
      # As this is cross-namespace communication, you also need the namespaceSelector.
      podSelector: {}
      # -- Specifies the namespaces which are allowed to access the http port
      namespaceSelector: {}
    alertmanager:
      # -- Specify the alertmanager port used for alerting
      port: 9093
      # -- Specifies the alertmanager Pods.
      # As this is cross-namespace communication, you also need the namespaceSelector.
      podSelector: {}
      # -- Specifies the namespace the alertmanager is running in
      namespaceSelector: {}
    externalStorage:
      # -- Specify the port used for external storage, e.g. AWS S3
      ports: []
      # -- Specifies specific network CIDRs you want to limit access to
      cidrs: []
    discovery:
      # -- Specify the port used for discovery
      port: null
      # -- Specifies the Pods labels used for discovery.
      # As this is cross-namespace communication, you also need the namespaceSelector.
      podSelector: {}
      # -- Specifies the namespace the discovery Pods are running in
      namespaceSelector: {}
