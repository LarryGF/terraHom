kube-prometheus-stack:
  kube-state-metrics:
    nodeSelector:
      priority: ${priority}
  resources:
    requests:
      cpu: 15m
      memory: 105M
    limits:
      cpu: 15m
      memory: 105M
  prometheusOperator:
    nodeSelector:
      priority: ${priority}
    admissionWebhooks:
      enabled: false
      patch:
        enabled: false
        nodeSelector:
          priority: ${priority}
    resources:
      requests:
        cpu: 15m
        memory: 105M
      limits:
        cpu: 15m
        memory: 105M
    prometheusConfigReloader:
      resources:
        requests:
          cpu: 11m
          memory: 53M
        limits:
          cpu: 11m
          memory: 53M
  kubeControllerManager:
    enabled: true

    ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on
    ##
    endpoints:
    - ${master_ip}
    # - 10.141.4.22
    # - 10.141.4.23
    # - 10.141.4.24

    ## If using kubeControllerManager.endpoints only the port and targetPort are used
    ##
    service:
      enabled: true
      ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change
      ## of default port in Kubernetes 1.22.
      ##
      port: null
      targetPort: null
      # selector:
      #   component: kube-controller-manager

    serviceMonitor:
      enabled: true
  
  kubeProxy:
    enabled: true

    ## If your kube proxy is not deployed as a pod, specify IPs it can be found on
    ##
    endpoints:
    - ${master_ip}
    # - 10.141.4.22
    # - 10.141.4.23
    # - 10.141.4.24

    service:
      enabled: true
      port: 10249
      targetPort: 10249
      # selector:
      #   k8s-app: kube-proxy

    serviceMonitor:
      enabled: true
  
  kubeScheduler:
    enabled: true

    ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on
    ##
    endpoints: 
    - ${master_ip}
    # - 10.141.4.22
    # - 10.141.4.23
    # - 10.141.4.24

    ## If using kubeScheduler.endpoints only the port and targetPort are used
    ##
    service:
      enabled: true
      ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change
      ## of default port in Kubernetes 1.23.
      ##
      port: null
      targetPort: null
      # selector:
      #   component: kube-scheduler

    serviceMonitor:
      enabled: true
  
  alertmanager:
    alertmanagerSpec:
      nodeSelector:
        priority: ${priority}
      resources:
        requests:
          cpu: 11m
          memory: 53M
        limits:
          cpu: 11m
          memory: 53M
  
    config:
      global:
        resolve_timeout: 5m
      inhibit_rules:
        - source_matchers:
            - 'severity = critical'
          target_matchers:
            - 'severity =~ warning|info'
          equal:
            - 'namespace'
            - 'alertname'
        - source_matchers:
            - 'severity = warning'
          target_matchers:
            - 'severity = info'
          equal:
            - 'namespace'
            - 'alertname'
        - source_matchers:
            - 'alertname = InfoInhibitor'
          target_matchers:
            - 'severity = info'
          equal:
            - 'namespace'
      route:
        group_by: ['namespace']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 12h
        receiver: 'discord-receiver'
        routes:
        - receiver: 'blackhole'
          matchers:
            - alertname =~ "InfoInhibitor|Watchdog|NodeClockNotSynchronising"
      receivers:
        - name: 'discord-receiver'
          webhook_configs:
            - url: 'http://kube-prometheus-stack-alertmanager-discord:9094'
        - name: 'blackhole'
      templates:
      - '/etc/alertmanager/config/*.tmpl'
      
    ingress:
      enabled: true
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt
        traefik.ingress.kubernetes.io/router.entrypoints: websecure
        gethomepage.dev/enabled: "true"
        gethomepage.dev/name: "Alertmanager"
        gethomepage.dev/description: "The Alertmanager handles alerts sent by client applications such as the Prometheus server"
        gethomepage.dev/group: "Monitoring"
        gethomepage.dev/icon: "alertmanager.png"
      hosts:
        - "alertmanager.${domain}"
      paths:
        - "/"
      tls:
        - hosts:
            - "alertmanager.${domain}"
          secretName: tls-alertmanager-ingress

  prometheus:
    ingress:
      enabled: true
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt
        traefik.ingress.kubernetes.io/router.entrypoints: websecure
        gethomepage.dev/enabled: "true"
        gethomepage.dev/name: "Prometheus"
        gethomepage.dev/description: "Monitoring system with a dimensional data model, flexible query language, efficient time series database and modern alerting approach"
        gethomepage.dev/group: "Monitoring"
        gethomepage.dev/icon: "prometheus.png"
        gethomepage.dev/widget.type: "prometheus"
        gethomepage.dev/widget.url: "http://prometheus-prometheus.${namespace}.svc.cluster.local:9090"
      hosts:
        - "prometheus.${domain}"    
      paths: 
        - "/"
      tls:
        - hosts:
            - "prometheus.${domain}"
          secretName: tls-prometheus-ingress
    prometheusSpec:
      nodeSelector:
        priority: ${priority}
      ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
      ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
      ## as specified in the official Prometheus documentation:
      ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are
      ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
      ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
      ## scrape configs are going to break Prometheus after the upgrade.
      ## AdditionalScrapeConfigs can be defined as a list or as a templated string.
      ##
      ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the
      ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
      ##
      additionalScrapeConfigs: []
      # - job_name: kube-etcd
      #   kubernetes_sd_configs:
      #     - role: node
      #   scheme: https
      #   tls_config:
      #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca
      #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client
      #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
      #   relabel_configs:
      #   - action: labelmap
      #     regex: __meta_kubernetes_node_label_(.+)
      #   - source_labels: [__address__]
      #     action: replace
      #     targetLabel: __address__
      #     regex: ([^:;]+):(\d+)
      #     replacement: ${1}:2379
      #   - source_labels: [__meta_kubernetes_node_name]
      #     action: keep
      #     regex: .*mst.*
      #   - source_labels: [__meta_kubernetes_node_name]
      #     action: replace
      #     targetLabel: node
      #     regex: (.*)
      #     replacement: ${1}
      #   metric_relabel_configs:
      #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)
      #     action: labeldrop
      #
      ## If scrape config contains a repetitive section, you may want to use a template.
      ## In the following example, you can see how to define `gce_sd_configs` for multiple zones
      # additionalScrapeConfigs: |
      #  - job_name: "node-exporter"
      #    gce_sd_configs:
      #    {{range $zone := .Values.gcp_zones}}
      #    - project: "project1"
      #      zone: "{{$zone}}"
      #      port: 9100
      #    {{end}}
      #    relabel_configs:
      #    ...



alertmanager-discord:
  image:
    repository: rogerrum/alertmanager-discord
    tag: 1.0.5
    pullPolicy: IfNotPresent

  # -- environment variables. See [image docs](https://github.com/metalmatze/alertmanager-bot) for more configuration options.
  env:
    # -- Discord Username
    DISCORD_USERNAME: "Alertmanager"
    DISCORD_WEBHOOK: "${api_key}"
  #   -- Discord Avatar icon
  #  DISCORD_AVATAR_URL: "https://avatars3.githubusercontent.com/u/3380462"
  #   -- Enable Verbose mode (log request and responses)
  #  VERBOSE: "ON"
  #   -- Discord Webhook - Can be used from envValueFrom secrets
  #  DISCORD_WEBHOOK:
  #    valueFrom:
  #      secretKeyRef:
  #        name: alertmanager-discord-secret
  #        key: DISCORD_WEBHOOK


  # -- Configures service settings for the chart. Normally this does not need to be modified.
  service:
    main:
      enabled: true
      ports:
        http:
          port: 9094

  ingress:
    main:
      enabled: false

  # -- Configure persistence settings for the chart under this key.
  # @default -- See values.yaml
  persistence:
    data:
      enabled: false
  
  resources:
    requests:
      cpu: 15m
      memory: 105M
    limits:
      cpu: 15m
      memory: 105M
